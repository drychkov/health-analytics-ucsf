{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analytics in Healthcare\n",
    "\n",
    "## Week 4 - Introduction to Spark\n",
    "\n",
    "Objective: To learn about the Apache Spark platform and common abstractions and work with a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.0\n",
    "      /_/\n",
    "\n",
    "Using Python version 2.7.6 (v2.7.6:3a1db0d2747e, Nov 10 2013 00:42:54)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "# print Spark version\n",
    "print(\"pyspark version: \" + str(sc.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once you run the PySpark kernel, it defines a Spark context object (sc) and a Spark SQL Session class (SparkSession)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext object at 0x7fc4a85d9490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession object at 0x7fc4a7fbf6d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = SparkSession(sc)\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Spark - RDD operations\n",
    "\n",
    "The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel.\n",
    "\n",
    "There are many ways to initialise an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:475"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize(range(1,1000))\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Using ``` take ``` one can select the first few elements of an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# take\n",
    "\n",
    "x = sc.parallelize([1,2,3,4,5])\n",
    "y = x.take(num = 3)\n",
    "print(y)\n",
    "\n",
    "# first\n",
    "y = x.first()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```rdd.collect()``` converts a RDD object into a Python list on the host machine. i.e get all the values in an RDD. If the size of the values of the RDD is greater that the capacity of the host machine, this could result in failure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# collect\n",
    "\n",
    "x = sc.parallelize([1,2,3,4,5])\n",
    "y = x.collect()\n",
    "print(type(y))\n",
    "print(y)  # not distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter\n",
    "y = x.filter(lambda x: x%2 == 1)  # filters out even elements\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: MapReduce\n",
    "\n",
    "There are 2 basic functions in the MapReduce framework - **Map** and **Reduce**. The operations of Map & Reduce deal with two types: the type A of input data being mapped, and the type B of output data being reduced.\n",
    "\n",
    "The **Map** function takes a series of key/value pairs, processes each, and generates zero or more output key/value pairs. The input and output types of the map can be (and often are) different from each other. Map operation takes individual values of type A and produces, for each a:A a value b:B; \n",
    "\n",
    "**Reduce** operation requires a binary operation â€¢ defined on values of type B; it consists of folding all available b:B to a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n",
      "[1, 1, 2, 4, 3, 9, 4, 16, 5, 25]\n"
     ]
    }
   ],
   "source": [
    "# map\n",
    "\n",
    "y = x.map(lambda x: (x,x**2))\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "\n",
    "\n",
    "# Flatmap - a function similar to map, introduced in Spark to allow a single list intead of list of tuples\n",
    "\n",
    "# flatMap\n",
    "y2 = x.flatMap(lambda x: (x, x**2))\n",
    "print(y2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# reduce\n",
    "\n",
    "y = x.reduce(lambda obj, accumulated: obj + accumulated)  # computes a cumulative sum\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 12), ('B', 3)]\n"
     ]
    }
   ],
   "source": [
    "# reduceByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "y = x.reduceByKey(lambda v1, v2: v1 + v2)\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Example - Word Count\n",
    "\n",
    "Word Count is the \"Hello, World!\" of distributed data processing frameworks. We can readReading from Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textRDD = sc.textFile(\"austen-sense.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line is a separate element in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [Sense and Sensibility by Jane Austen 1811]\n",
      "1 \n",
      "2 CHAPTER 1\n",
      "3 \n",
      "4 \n",
      "5 The family of Dashwood had long been settled in Sussex.\n",
      "6 Their estate was large, and their residence was at Norland Park,\n",
      "7 in the centre of their property, where, for many generations,\n",
      "8 they had lived in so respectable a manner as to engage\n",
      "9 the general good opinion of their surrounding acquaintance.\n"
     ]
    }
   ],
   "source": [
    "for line_no, element in enumerate(textRDD.take(10)):\n",
    "    print line_no, element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14796"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can check the number of lines the text file has\n",
    "textRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split each line into words\n",
    "wordRDD = textRDD.flatMap(lambda line: line.split(\" \"))\n",
    "print(wordRDD.take(5))\n",
    "\n",
    "# Map each word to a single number\n",
    "mappedwordsRDD = wordRDD.map(lambda word: (word, 1))\n",
    "\n",
    "# Reduce by each word to find the word count of each number\n",
    "wordCounts = mappedwordsRDD.reduceByKey(lambda word, count: word + count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Saving to Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`.saveAsTextFile()` saves an RDD as a string. there is also `.saveAsPickleFile()`.\n",
    "\n",
    "`.rsaveAsNewAPIHadoopDataset()` saves an RDD object to HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Create a function\n",
    "\n",
    "Create a function to accept a text file, and save a text file with the name \"source_file_word_count.txt\" with the word counts. Assume that the text file is too large to fit into memory, or the Hard Disk of your computer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_count(...):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark - PySpark",
   "language": "python",
   "name": "spark_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.12\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
